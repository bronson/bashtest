## TODO

This file is a backlog of ideas. Here they are, in no particular order...

---

Two most important changes now that it's clear that requiring tests to clean up after themselves is too overbearing:
- make it so tests don't need to clean up (and add a test for cleaning upwrite-protected files)
- make it so tests don't need to use a framework assert to be considered a pass. No error? Pass.

---

In the interest of explicit dependencies (see "should tests clean up after themselves"),
we need to make it easier to embed files inside tests. Needs to be pretty, easy to edit,
and syntax highlighted.

Bash indented heredocs (<<-EOF) don't work because they need to be indented using tabs. That's
not easy to edit.

Good suggestions here: https://stackoverflow.com/questions/33815600/indenting-heredocs-with-spaces

---

1. Add pretty heredocs and helpers to create files (with automatic cleanup).
2. Make it an error when a test leaves any files in its rundir or cache dir.
3. Then randomize the order of the tests, both files and tests in the file.

---

I think I should remove the cache dir? It's an antipattern, and encourages people to use it
in places they probably shouldn't. If someone really needs it, they
can create something like it on their own using the before and after hooks.

---

I really really want to make the skip and fail functions stop execution immediately,
much like abort.

Yes, I think any test should only be able to fail once (unless you pass an arg or
set something that allows multiple failures). Getting 12 error messages for one
mistake is hardly ever helpful.

Implementation: run all tests inside a single subshell. If any of the tests
abort, skip, or fail, they exit. The parent handles the issue and, if it's not
a fail, resumes the testing with the next test.

---

It's becoming clear that people should be able to write tests with their own assertions.

For example, this seems like a perfectly good test:

test:simple() {
    echo content | create_file test1.txt
    [ -f test1.txt ] || abort "test1.txt must be created."
}

But the test runner thinks that test was skipped! No built-in assertions were made,
so nothing happened, right?  (wrong.)

We should count tests that made zero assertions as having been run, not as having
been skipped.

That's fair, because I don't want to force someone to understand our esoteric API
to write a simple test. They should be able to sit down and start writing Bash
and produce something that works.

AND IF ALL THIS IS TRUE, then we should probably remove the assertion counter too.
That's going to be seriously misleading when people are doing their own assertions.

---

The hardest question to answer so far is: "Should tests clean up after themselves?"

First, let's consider two simpler questions:
* Should tests always be run in random order?
* Should tests be required to declare their dependencies?

I think the answer to both of these is a clear "yes".   (if random test order leads to slow
tests (say, due to bad cache usage), maybe there should be an escape hatch ... but a better
solution would be to explicitly group tests that want to reuse the same cached data)

Implicit dependencies might be the biggest cause of brittle code bases.
If you can't move a chunk of code from one place to the other without breaking
unknowable things in far places (because there were a lot of hidden dependencies),
you have bad code.

When Bashtest was first written, tests that didn't clean up after themselves would fail.
That became irritating when the tests started using lots of files, so that requirement was
relaxed. Tests can just slop up the directories and everything gets rm-rf'd at the end.

This has led to the opposite extreme: now tests can accidentally rely on activity that
happened far away.

SO, the answer is to make sure that dependencies are explicit. If enough deps are explicit,
then there's no problem with running tests in any order.

And, if it's easy to write tests that clean up after themselves, then people won't even
want to write code that leaves slop around. The problem might just be bad tools.

SO: we need test helpers that make sure the files they create are cleaned up.
And, if people want, they can add an after:all hook that will rm-rf after each
test.

I think it's time to enforce fastidious tests.

---

Depending on research, make it possible to use the after:each hook to either
clean up after each test, or if cleanup is automatic, to fail if any tests leave
files behind (in case the project wants to be more pedantic).

---

We need to have an option to run tests in random order to ensure order dependency
isn't a thing..
We should also make an option to force each test into its own subshell to verify
that tests aren't relying on other tests.

---

The benchmark should always be enabled. It hardly adds any overhead.

(or should it? Sometimes computers are 2X slower for no good reason, just some
temporary indexing or something, and I don't want users to panic over nonissues.
Often less information is more.)

---

maybe subtests need a stronger solution, where they run a different
./run-tests executable from the global one, so they get a different config
file too. Or some other way of clearing the config file?

---

add more tests for nested excluded directories

---

Now that we have a config file, we probably need to be able to specify any
number of before:each, before:all etc.

---

Make sure before:all runs once before testfiles are forked, and before:each
runs in each subshell, not affecting other subshells.

---

in addition to missing testfiles, what if I don't have permission to read a testfile?

---

verify tsk_launched_file can only ever be a testfile, then rename it to tsk_launched_testfile

---

why is the abort message in test:missing_testfiles so weird looking?

---

Add the reason the test was skipped to skip tests

Test the skip keyword

---

How do I keep run-tests out of the fixtures directory?
(probably associated with having some sort of local config file)

---

Indented heredocs are unusable these days because they require tabs.
Try using https://stackoverflow.com/a/33817423

---

How can I test the pause function?

---

pluralize callers almost always prints the number first, so just have
pluralize print the number.

---

Write an example before:each config that loads config files hierarchically
starting from the testfile and moving back up to the root dir. That
way each directory can share a config.

---

Get all usage of tsk_ variables out of testfiles. It's too dirty.

---

ensure tests are started in the run directory
and that if a previous test cd'd, the next test still starts in the rundir.

---

mocking in before:all will apply to every test

---

BENCHMARK
does it make sense to reset stdout/stderr and then set it again immediately?
would it save any time to remove the exec 1>&3 2>&4 reset?

BENCHMARK
Can we run each test in its own subshell?
It would totally fix tests accidentally sharing variables with each other.

---

Skipping tests...

skip: instead of test: to skip tests?  Downside is that all the skips
will be out of order. But that's fine.

or add a 'skip' call? Or just return to skip?   return # skip

or both? What's the best way to skip?

---

Allow mocking in before:all

---

does it make sense to have expect_error? Should probably just run
the test in a sub-run-tests and expect the result.
ANSWER: I think no, it doesn't make sense.

---

run all tests under -e by default?

---

        # TODO: how do we preserve failed tests?
        # Since the test failed, rename the run directory to preserve the state
        # local safe_name=$(echo "$tsk_test_name" | tr ' /\\:*?"<>|' '_')
        # local failed_dir="$tsk_root_dir/failed-$safe_name"
        # mv "$tsk_run_dir" "$failed_dir"

        # # Move artifacts to the failed test directory with clear names
        # mkdir -p "$failed_dir"
        # mv "$tsk_artifacts_dir/test-stdout" "$failed_dir/FAILED-stdout.txt" 2>&4
        # mv "$tsk_artifacts_dir/test-stderr" "$failed_dir/FAILED-stderr.txt" 2>&4
        # if [ -d "$tsk_artifacts_dir/test-mocks" ]; then
        #     mv "$tsk_artifacts_dir/test-mocks" "$failed_dir/FAILED-mocks" 2>&4
        # fi

---

# TODO: tsk_testfile_path implies it's the path to the file, not the file itself.

---

Run shellcheck

TODO: ensure tests emitting stderr is an error
TODO: ensure tests leaving files or directories in the test dir is an error
TODO: ensure tests leaving functions or variables behind is an error

---

Should run all tests in parallel, even inside a single test file. Creating
and destorying directories is so fast, there's no point to doing things
serially.

---

ensure the test directory is chmod 700 so attackers can't access in-progress tests

---

What about the brief amount of time when the computer is starting up where the service might be exposed but not protected by iptables yet?

We need to be 100% sure we sequence the startup so there's no chance of accessing the live service without iptables over it. There's the chance an attacker might open a connection in that tiny sliver of time.  (even if they do, the connection will just stall when the portkey protections go into place, right? So, short of an APT situation where they can control your power and restart your computer over and over until they find their way in, it's not THAT big a deal?)


---

We can't run each test in a subshell because that makes tests run 1000 times slower.

It takes even longer to give each test its own test directory
than it does to run each test in its own subshell. We definitely
should reuse test directories the way we do now.

---------


Testing:

Should probably only preserve test results when running a single file or
a single test. If running all tests, don't pollute the directory.
Hope the test harness printed enough to solve the issue anyway.



Add before:each, before:all, after:each, after:all
