#!/bin/bash

# Testing Shell Kit

# Guiding Principles
#
#  - Tests are meant to look and act like bash code. Familiarity is key.
#  - Testing is as self-contained as possible, nothing to install on the host.
#  - Test output should contain enough information to see what went wrong.
#  - the API should be clean and readable; no need to use things named _ or tsk_.
#
# Tests are run with an empty writeable directory as the CWD. They
# are expected to leave this directory empty when they're finished,
# otherwise the test fails. TODO
#
# If your test produces output on stderr, make sure to check it,
# otherwise the test will fail. If you want to
# ignore stderr, you can do something like TODO: `stderr_matches .`
#
#: ## Output Formatters
#:
#: The test runner supports different output formats via the TEST_FORMATTER environment variable:
#:
#: - list: use only when running a single testfile, shows each test
#: - dot: multithread-friendly, used when running multiple testfiles
#: - test: a simple formant meant for writing tests

#: ## The Test Environment
#:
#: ### directories
#:
#: When a test is running, the current working directory is set
#: to an empty scratch directory. The test should make no assumption
#: about where this directory is stored; it might be in your project
#: directory, it might be in /tmp, and it might be on a magic ramdisk
#: somewhere.
#:
#: Your test can also access these directories:
#: - testfile_dir: The directory containing the testfile of the test currently being run.
#:   For example, if you
#:   have some sample data in a file next to the testfile, you can
#:   specify it like this: `$(testfile_dir)/sample_data.txt`
#: - framework_dir: The directory containing the run-tests script.
#:   If you have a helper script in the same directory as the run-tests
#:   script, you can include it like this: `$(framework_dir)/helper.sh`
#: - cache_dir: A temporary directory that is created when tests start
#:   and removed when tests complete successfully. This directory is shared
#:   across all tests and can be used for caching data between test runs.
#:   Access it like this: `$(cache_dir)/my-data.txt`
#:
#: ### Lifecycle Hooks
#:
#: If your testfile defines any of the folllowing hooks, they'll be called during test execution:
#:
#: - before:all: Runs once before any tests are executed.
#: - before:each: Runs before each test, receives the test name as an argument.
#: - after:each: Runs after each test, receives the test name as an argument. Can perform assertions on the test output.
#: - after:all: Runs once after all tests have completed.
#:
#: ## The Testfile
#:
#: Testfiles start by including the test framework:
#:
#: ```bash
#: source "$(dirname "$BASH_SOURCE")/../run-tests.sh"
#: ```

if [ -n "$tsk_framework_file" ]; then
    # test harness is already loaded, don't load again.
    return # calling exit would terminate the test currently in progress.
fi


#
#     The Global Environment
#

# TODO: ensure we don't leak any variables into the test other than those named here

# we reserve the tsk_ prefix to prevent incurring any conflicts
# with variables in the tests themselves. TODO: can we test if we
# accidentally leak any variables or non-api functions into the testfiles?

# Store the path to the run-tests.sh script for later use
tsk_orig_cwd="$(pwd)"     # save the directory we were started in
tsk_framework_file="$(readlink -f "${BASH_SOURCE[0]}")"

# if the user ran a testfile directly (`./01.test`), tsk_launched_file will be that file
tsk_launched_file=""
if [ "${BASH_SOURCE[0]}" != "$0" ]; then
    tsk_launched_file="${BASH_SOURCE[1]}"
fi

# tests have their output redirected to files so we need to:
exec 3>&1 4>&2 # save stdout and stderr to FDs 3 and 4

# TODO: this is a misnomer... we also use tsk_use_color to indicate
# if we should use control codes to do curses-style terminal output
if [ -z "${tsk_use_color}" ]; then
    tsk_use_color=false
    if [ -t 3 ]; then
        tsk_use_color=true
    fi
fi

# Colors for output
# TODO: get these colors out of the test global scope
if [ "$tsk_use_color" = true ]; then
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    NC='\033[0m' # No Color
    CR=$'\r'
else
    RED=''
    GREEN=''
    YELLOW=''
    NC=''
    CR=''
fi


#
#    The API available to tests...
#

_compare() {
    local op=$1 expected=$2 actual=$3

    tsk_test_assertions=$((tsk_test_assertions + 1))
    if [ ! x"$expected" "$op" x"$actual" ]; then
       _add_error "line $(_line_number 2): ${FUNCNAME[1]} expected '$expected', got '$actual'"
    fi
}

is_eq() { _compare '=' "$1" "$2"; }
is_neq() { _compare '!=' "$1" "$2"; }

# TODO: file_contains? stdout_contains?
stderr_contains() {
    local expected_text="$1"

    tsk_test_assertions=$((tsk_test_assertions + 1))
    tsk_stderr_checked=true

    if ! grep -q "$expected_text" "$tsk_root_dir/test-stderr"; then
        _add_error_with_stderr "stderr_contains line $(_line_number): stderr doesn't contain: '$expected_text'"
    fi
}

pause() {
    # TODO: make sure other tests aren't running simultaneously
    # TODO: export relevant variables and (export -f) functions
    # TODO: if the interactive shell exits with an error, stop testing.

    local prompt_msg="${1:-Test paused. Type 'exit' to continue.}"

    exec 5>&1 6>&2  # save the current test's stdout/stderr
    exec 1>&3 2>&4  # Restore original stdout/stderr for interactive use

    printf "\n\n\n=== PAUSED $tsk_test_name at line $(_line_number) ===\n"
    printf "$prompt_msg\n\n"
    PS1="[PAUSED] \w $ " bash --norc -i
    printf "=== RESUMING TEST ===\n\n"

    exec 1>&5 2>&6
}

# One trailing newline, if present, is trimmed from stdout before comparison.
# If you need to be pedantically correct about the presence of the final
# newline, you'll have to check for it outside of this function.
# TODO: this is not great.
file_is() {
    # TODO: this should be a utility function, then file_is calls it, matching stdout_is etc
    local expected_text="$1"
    local file_path="$2"
    local call_name="${3:-file_is}"
    local lineno="${4:-$(_line_number)}"

    # if the expected text isn't in the argument, it's on stdin
    if [ -z "$expected_text" ]; then
        if read -t 0; then    # don't hang if data isn't there
            expected_text="$(cat)"
        fi
    fi

    tsk_test_assertions=$((tsk_test_assertions + 1))

    if [ -z "$file_path" ] || [ ! -f "$file_path" ]; then
        _add_error "line $lineno: $call_name couldn't open '$file_path'"
        return
    fi

    local actual_text="$(cat "$file_path")"
    if [ "$expected_text" != "$actual_text" ]; then
        _add_error "line $lineno: $call_name $(_format_diff "$expected_text" "$actual_text")"
    fi
}

stdout_is() {
    file_is "$1" "$tsk_root_dir/test-stdout" stdout_is "$(_line_number)"
}

stderr_is() {
    file_is "$1" "$tsk_root_dir/test-stderr" stderr_is "$(_line_number)"
    tsk_stderr_checked=true
}

abort() {
    # TODO: could run all tests in a subshell that keeps testing until abort is called.
    # Abort exits the subshell and then the tests end normally.
    local errmsg="line $(_line_number): $1"
    printf "${CR} %02d ${RED}ABORT${NC}  $tsk_test_name $errmsg\n" "$tsk_total_count" >&3
    printf "${RED}TEST ABORTED${NC}\n" >&3
    exit 127
}

mock() {
    local command="$1"
    local behavior="$2"
    local mock_dir="$tsk_root_dir/test-mocks"
    local executable="$mock_dir/$command"

    # Create the mock directory and add it to PATH if needed
    if [ ! -d "$mock_dir" ]; then
        mkdir -p "$mock_dir" 2>&4 || exit 1
    fi

    # Ensure we have an absolute path for the mock directory for PATH
    mock_dir="$(readlink -f "$mock_dir")"
    executable="$mock_dir/$command"

    # Add mock dir to PATH only when the first mock is created
    if [[ ":$PATH:" != *":$mock_dir:"* ]]; then
        export PATH="$mock_dir:$PATH"
    fi

    cat > "$executable" << EOF
#!/bin/bash
$behavior
EOF
    chmod +x "$executable"
}

# Returns the directory of the currently running testfile
testfile_dir() {
    # TODO/TOTEST: what if the test is coming from stdin? dirname of STDIN?
    if [ -z "$tsk_testfile" ]; then
        echo "Error: testfile_dir was called without a testfile!" >&4
        exit 1
    fi
    dirname "$tsk_testfile"
}

# Returns the directory of the run-tests script
framework_dir() {
    dirname "$tsk_framework_file"
}

# Returns the shared cache directory that is available to all tests in the current testfile
cache_dir() {
    echo "$tsk_root_dir/cache"
}

# call this to make differences be displayed using `diff` rather than side-by-side
# TODO: this needs to be improved!
want_diff() {
    tsk_test_want_format="diff"
}

# call this when writing tests to dump its arguments immediately to the screen
debug() {
    echo "$@" >&3
}

# Called in a pre-hook or during the test, causes this test to be skipped.
# Note that it can't actually exit the test so `skip; return` is a useful idiom.
skip() {
    tsk_skip_test=true
}

pluralize() {
    local count=$1
    local singular=$2
    local plural=${3:-${singular}s}

    if [ "$count" -eq 1 ]; then
        echo "$singular"
    else
        echo "$plural"
    fi
}


#
#    Internal code to run the tests
#

# accepts testfiles and directories
_add_argument() {
    # TODO: ensure the user can only specify stdin once?
    if [ -d "$1" ]; then
        while IFS= read -r file; do
            testfiles+=("$file")
        done < <(find "$1" -type f \( -name "*.test" -o -name "*.skip" \) | sort)
    else
        testfiles+=("$1")
    fi
}

_process_arguments() {
    testfiles=()
    max_jobs=12  # not the fastest but should be reasonably close

    # If the test harness is being sourced from a testfile, add the testfile.
    if [ -n "$tsk_launched_file" ]; then
        testfiles+=("$tsk_launched_file")
    fi

    # TODO: if a directory is specified, add all .test files in that directory
    # TODO: should we care about whether a testfile argument ends in '.test'?
    while (( "$#" )); do case $1 in
        -d|--dir) DIR="$2"; shift; shift;;
        -f|--formatter) tsk_test_formatter="$2"; shift; shift;;
        --formatter=*) tsk_test_formatter="${1#*=}"; shift;;
        -j|--jobs) max_jobs="$2"; shift; shift;;
        --jobs=*) max_jobs="${1#*=}"; shift;;
        -k|--keep) tsk_keep_artifacts=true; shift;;
        -n|--name|--number) NAME="$2"; shift; shift;; # TODO
        -w|--watch) WATCH=true; shift;;               # TODO
        *) _add_argument "$1"; shift;;
    esac; done

    # ensure max_jobs is a positive integer
    if [[ ! "$max_jobs" =~ ^[0-9]+$ ]]; then
        echo "invalid max jobs: '$max_jobs'"
        exit 1
    fi

    # If no test files were specified, run all .test files recursively
    if [ ${#testfiles[@]} -eq 0 ]; then
        _add_argument '.'
    fi

    if [ ${#testfiles[@]} -eq 0 ]; then
        echo "No testfiles found. Testfile names must end in '.test'." >&2
        exit 120
    fi
}

# returns the line number of the caller of the given frame
# defaults to the caller of the function calling _line_number
_line_number() {
    local frame=${1:-1}
    local info=$(caller "$frame")
    echo "${info%% *}"
}

# used to quote large content in the test output, like stderr
_blockquote() {
    echo -n "$1" | sed 's/^/    | /'
}

_format_diff() {
    local expected="$1"
    local actual="$2"

    # For short single-line content, always use simple quoted format
    if [[ "$expected" != *$'\n'* && "$actual" != *$'\n'* &&
          ${#expected} -lt 30 && ${#actual} -lt 30 ]]; then
        echo "expected '$expected', got '$actual'"
        return
    fi

    # if the user asked for a diff, provide it
    if [ "$tsk_test_want_format" = "diff" ] && command -v diff &>/dev/null; then
        local diff_output="$(diff -u <(echo "$expected") <(echo "$actual") | tail -n +3 | cat -vet)"
        if [[ -n "$diff_output" ]]; then
            echo "diff:"$'\n'"$(_blockquote "$diff_output")"
            return
        fi
    fi

    echo "expected:"$'\n'"$(_blockquote "$expected")"$'\n'"    but got:"$'\n'"$(_blockquote "$actual")"
}

# todo? make line number and function name automatic?
_add_error() {
    tsk_test_error_messages+=("$1")
}

# same as _add_error, but includes a few lines of stderr after the message
_add_error_with_stderr() {
    local message="$1"
    local snippet="$(head -n 20 "$tsk_root_dir/test-stderr")"
    message="$message"$'\n'"$(_blockquote "$snippet")"
    _add_error "$message"
}

# used to test the API assertion calls: if an assertion failed,
# but was expected, then the failure is cleared and the test passes.
_expect_error() {
    local expected_message="$1"

    tsk_test_assertions=$((tsk_test_assertions + 1))

    # if nothing threw an error, that's an error.
    if [ ${#tsk_test_error_messages[@]} -eq 0 ]; then
        _add_error "line $(_line_number): _expect_error - expected error message: '$expected_message', but no errors were reported"
        return
    fi

    # Get the most recent error message
    local last_idx=$((${#tsk_test_error_messages[@]} - 1))
    local last_message="$(echo "${tsk_test_error_messages[last_idx]}" |
        sed -E 's/line [0-9]+:/line nnn:/g')"

    if [ "$expected_message" = "$last_message" ]; then
        # the error was expected so it gets cleared
        unset "tsk_test_error_messages[last_idx]"
    else
        # the error we expected was not the error we got
        tsk_test_error_messages[last_idx]="line $(_line_number): _expect_error $(_format_diff "$expected_message" "$last_message")"
    fi
}

# at the end of the test, all mocks are removed
# TODO: one day we might want to set up mocks once and
# then run them in multiple tests.
_cleanup_mocks() {
    rm -rf "$tsk_root_dir/test-mocks"/* "$tsk_root_dir/test-mocks"/.*
}

_check_for_abandoned_files() {
    # Count all files (including hidden ones)
    local file_count=$(find . -mindepth 1 -maxdepth 1 | wc -l)

    # If any files found, add an error
    if [ "$file_count" -gt 0 ]; then
        local files_list=$(find . -mindepth 1 -maxdepth 1 -printf "%f\n" | sort)
        _add_error "test didn't clean up $(pluralize "$file_count" 'file'): $files_list"
    fi
}


# Generates a list of test functions to run.
_find_tests() {
    # `declare -F | cut -d" " -f3 | grep '^test:'`
    # `typeset -F` seems to be a synonym for declare F
    compgen -A function test:
    compgen -A function skip:
    # TODO: should run the tests in the order they're declared
    # in the file, not in alphabetical order.
}

_print_test_error_messages() {
    if [ ${#tsk_test_error_messages[@]} -gt 0 ]; then
        for msg in "${tsk_test_error_messages[@]}"; do
            echo "  - $msg" >&3
        done
    fi
}

_test_formatter_pre_test() { :; }

_test_formatter_post_test() {
    echo "$1 $tsk_test_name" >&3
    _print_test_error_messages
}

_test_formatter_test_summary() { :; }

_dot_formatter_pre_test() { :; }

_dot_formatter_post_test() {
    case "$1" in
        pass) echo -n '.' >&3 ;;
        skip) echo -n 's' >&3 ;;
        fail) echo -n 'F' >&3 ;;
        *) echo -n 'E' >&3 ;;
    esac
}

_dot_formatter_test_summary() {
    echo # _dot_formatter_post_test doesn't print a final newline so do it here
}

# called before each test starts
_list_formatter_pre_test() {
    # only do this if we're pretty-printing to a terminal
    if [ -n "$CR" ]; then
        # no newline on this line, end_test will back up and print the result
        printf " %02d ....   $tsk_test_name" "$tsk_total_count" >&3
    fi
}

# called after each test is complete
_list_formatter_post_test() {
    local result
    case "$1" in
        pass) result="${GREEN}pass${NC}" ;;
        skip) result="${YELLOW}skip${NC}" ;;
        fail) result="${RED}FAIL${NC}" ;;
        *) result="${RED}INTERNAL ERROR: ${result}${NC}" ;;
    esac

    local errmsg=''
    local num_fails="${#tsk_test_error_messages[@]}"
    if [ "$num_fails" -gt 0 ]; then
        errmsg=" ($num_fails $(pluralize "$num_fails" "error"))"
    fi

    printf "$CR %02d $result   $tsk_test_name$errmsg\n" "$tsk_total_count" >&3
    _print_test_error_messages
}

_list_formatter_test_summary() {
    local color="$GREEN"

    local skip_msg=""
    if [ $tsk_skip_count -gt 0 ]; then
        color="$YELLOW"
        skip_msg=", ${tsk_skip_count} skipped"
    fi

    if [ $tsk_fail_count -gt 0 ]; then
        color="$RED"
    fi

    local count="${tsk_total_count} $(pluralize "$tsk_total_count" "test")"
    local assertions="${tsk_total_assertions} $(pluralize "$tsk_total_assertions" "assertion")."
    echo -e "${color}$count: ${tsk_pass_count} passed, ${tsk_fail_count} failed${skip_msg}.   ${assertions}${NC}" >&3
}

_run_test() {
    [ -n "$tsk_skip_test" ] && return
    exec 1>"$tsk_root_dir/test-stdout" 2>"$tsk_root_dir/test-stderr"

    if type before:each &>/dev/null; then
        before:each "$tsk_test_name"
    fi
    [ -n "$tsk_skip_test" ] && return

    "$tsk_test_name" "$tsk_test_name"   # TODO: useful arguments?
    if type after:each &>/dev/null; then
        after:each "$tsk_test_name"
    fi
    exec 1>&3 2>&4 # restore asap so test framework errors aren't swallowed

    # Add this test's assertions to the total
    tsk_total_assertions=$((tsk_total_assertions + tsk_test_assertions))

    _cleanup_mocks

    if [ -s "$tsk_root_dir/test-stderr" ] && [ "$tsk_stderr_checked" = false ]; then
        _add_error_with_stderr "test produced stderr:"
    fi

    # TODO
    # _check_for_abandoned_files
}

_compute_test_result() {
    if [ $tsk_test_assertions -eq 0 ]; then
        # Test made no assertions so call it skipped
        tsk_skip_count=$((tsk_skip_count + 1))
        tsk_test_result="skip"
    elif [ ${#tsk_test_error_messages[@]} -eq 0 ]; then
        # Test passed (made assertions and no errors)
        tsk_pass_count=$((tsk_pass_count + 1))
        tsk_test_result="pass"
    else
        # test had one or more failing assertions
        tsk_fail_count=$((tsk_fail_count + 1))
        tsk_test_result="fail"
    fi
}

_run_testfile_tests() {
    local tsk_testfile="$1"

    tsk_skip_all_tests=""
    if [[ "$tsk_testfile" == *.skip ]]; then
        tsk_skip_all_tests=true
    fi

    local tsk_test_want_format="" # TODO: this sucks.

    # load the testfile if it's not already loaded
    if [ "$tsk_testfile" != "$tsk_launched_file" ]; then
        # remove the launched testfile's test functions
        for func in $(compgen -A function test:); do
            unset -f "$func"
        done

        if [ "$tsk_testfile" = "-" ]; then
            source <(cat) # load the testfile from stdin
        else
            # convert to aboslute path because we'll be running the test
            # in a different directory than the cwd
            tsk_testfile="$(readlink -f "$tsk_testfile")"
            source "$tsk_testfile"
        fi
    fi

    mkdir -p "$tsk_root_dir/run" "$tsk_root_dir/cache" 2>&4 || exit 1
    cd "$tsk_root_dir/run"

    # Run the before:all hook if defined
    if [ -z $tsk_skip_all_tests ] && type before:all &>/dev/null; then
        # TODO: where should stdout/stderr go for before/after:all?
        # TODO: what if before:all fails some assertions?
        before:all
        # If skip was called in before:all, skip all tests in this file
        if [ -n "$tsk_skip_test" ]; then
            tsk_skip_all_tests=true
        fi
    fi

    local tsk_test_name
    for tsk_test_name in $(_find_tests); do
        # Reset test-specific variables for each test
        tsk_test_assertions=0
        tsk_test_error_messages=()
        tsk_stderr_checked=false
        tsk_test_result=""
        tsk_skip_test="$tsk_skip_all_tests"

        tsk_total_count=$((tsk_total_count + 1))
        "_${tsk_test_formatter}_formatter_pre_test"
        if [[ "$tsk_test_name" == test:* ]]; then
            _run_test "$tsk_test_name"
        fi
        _compute_test_result
        "_${tsk_test_formatter}_formatter_post_test" "$tsk_test_result"
    done

    if [ -z $tsk_skip_all_tests ] && type after:all &>/dev/null; then
        # TODO: where should stdout/stderr go for before/after:all?
        # TODO: what if after:all fails some assertions?
        after:all
    fi
}

# TODO: test running tests in the testfile in parallel
# testing hierarchy:
# - tsk_root_dir         root for this particular testfile
#   - cache              a directory shared by all tests in the testfile
#   - run                the current test's cwd, empty at start of test (TODO: parallel?)
#   - test-mocks         directory containing the current test's mocks
#   - test-stdout        file, the current test's stdout
#   - test-stderr        file, the current test's stderr

_prepare_test_environment() {
    tsk_total_count=0     # current number of tests run
    tsk_pass_count=0      # current number of tests passed
    tsk_fail_count=0      # current number of tests failed
    tsk_skip_count=0      # current number of tests skipped
    tsk_total_assertions=0 # total number of assertions made

    tsk_keep_artifacts=''

    # TODO: running in /tmp isn't good for tests that require giant files
    # TODO: option to create and mount a ramdisk to run tests

    tsk_root_dir="$(mktemp -d "/tmp/bashtest-XXXXXX")"
}

# given a testfile, ensure we return a unique name for this test
# even if other testfiles have the same identical name
_get_testfile_unique_dirname() {
    local testfile="$1"
    local filename="${testfile##*/}"     # remove leading directories
    filename="${filename%.test}"   # remove trailing ".test"
    echo "$filename--$testfile_count"
}

_run_multiple_testfiles() {
    running_jobs=0   # TODO _tsk these?
    testfile_count=0

    for testfile in "${testfiles[@]}"; do
        testfile_count=$((testfile_count + 1))
        if [ "$running_jobs" -ge "$max_jobs" ]; then
            wait -n
            running_jobs=$((running_jobs - 1))
        fi

        (
            tsk_root_dir="$tsk_root_dir/$(_get_testfile_unique_dirname "$testfile")"
            _run_testfile_tests "$testfile"
        ) &
        running_jobs=$((running_jobs + 1))
    done

    wait  # wait for the remaining jobs to finish
}

# there are many ways to run the tests:
# - Run multiple test files: ./run-tests file1.test file2.test
# - Run tests from stdin: cat test | ./run-tests -
# - Run a single test file: ./01-my-tests.test
# - You should also be able to run multiple tests: ./01-a.test 02-b.test
# - if you name a directory, all tests in that directory will be run.
# - if you don't name any directories or files, the cwd will be searched for tests.

_prepare_test_environment
_process_arguments "$@"
# if the user supplies a single testfile, we show a more verbose output than if they're running multiple
if [ ${#testfiles[@]} -eq 1 ]; then
    tsk_test_formatter="${tsk_test_formatter:-list}"
    _run_testfile_tests "${testfiles[0]}"
else
    # TODO: need to print failures and results when running multiple test files
    tsk_test_formatter="${tsk_test_formatter:-dot}"
    _run_multiple_testfiles
fi
if [ "$tsk_keep_artifacts" != true ]; then
    rm -rf "$tsk_root_dir"
fi
_${tsk_test_formatter}_formatter_test_summary
