#!/bin/bash

# Testing Shell Kit

# Guiding Principles
#
#  - Tests are meant to look and act like bash code. Familiarity is key.
#  - Testing is as self-contained as possible. There's nothing to install on the host.
#  - Test output should contain enough information to see what went wrong and what to do next.
#
# Tests are
# - run with an empty writeable directory as the CWD.
# - run in any order.
# - completely independent and isolated. It's an error if they rely on each other.
# - must not produce any output on stdout or stderr.
#
# If your test produces output on stderr, make sure to check it,
# otherwise the test will fail. If you want to
# ignore stderr, you can do something like TODO: `stderr_matches .`
#
#: ## Output Formatters
#:
#: The test runner supports different output formats via the TEST_FORMATTER environment variable:
#:
#: - list: use only when running a single testfile, shows each test
#: - dot: multithread-friendly, used when running multiple testfiles
#: - test: a simple formant meant for writing tests

#: ## The Test Environment
#:
#: ### directories
#:
#: When a test is running, the current working directory is set
#: to an empty scratch directory. The test should make no assumption
#: about where this directory is stored; it might be in your project
#: directory, it might be in /tmp, and it might be on an exotic ramdisk
#: somewhere.
#:
#: Your test can also access these directories:
#: - testfile_dir: The directory containing the testfile of the test currently being run.
#:   For example, if you
#:   have some sample data in a file next to the testfile, you can
#:   specify it like this: `$(testfile_dir)/sample_data.txt`
#: - framework_dir: The directory containing the run-tests script.
#:   If you have a helper script in the same directory as the run-tests
#:   script, you can include it like this: `$(framework_dir)/helper.sh`
#:   (and $(framework_dir)/test-config.sh is loaded automatically)
#:
#: ### Lifecycle Hooks
#:
#: If your testfile defines any of the folllowing hooks, they'll be called during test execution:
#:
#: - before:all: Runs once before any tests are executed.
#: - before:each: Runs before each test, receives the test name as an argument.
#: - after:each: Runs after each test, receives the test name as an argument. Can perform assertions on the test output.
#: - after:all: Runs once after all tests have completed.

if [ -n "$tsk_framework_file" ]; then
    # test harness is already loaded, don't load again.
    return # calling exit would terminate the test currently in progress.
fi


#
#     The Global Environment
#

# TODO: ensure we don't leak any variables into the test other than those named here

# we reserve the tsk_ prefix to prevent incurring any conflicts
# with variables in the tests themselves. TODO: can we test if we
# accidentally leak any variables or non-api functions into the testfiles?

# Store the path to the run-tests.sh script for later use
tsk_orig_cwd="$(pwd)"     # save the directory we were started in
tsk_framework_file="$(readlink -f "${BASH_SOURCE[0]}")"
tsk_framework_dir="$(dirname "$tsk_framework_file")"
tsk_repeat=1              # the number of times to repeat the entire test run

# if config files exclude any directories, they're stored here until argumets are processed
tsk_excluded_dirs=()

# if the user ran a testfile directly (`./01.test`), tsk_launched_file will be that file
tsk_launched_file=""
if [ "${BASH_SOURCE[0]}" != "$0" ]; then
    tsk_launched_file="${BASH_SOURCE[1]}"
fi

# TODO: this is a misnomer... we also use tsk_use_color to indicate
# if we should use control codes to do curses-style terminal output
if [ -z "${tsk_use_color}" ]; then
    tsk_use_color=false
    if [ -t 1 ]; then
        tsk_use_color=true
    fi
fi

# Colors for output
# TODO: get these colors out of the test global scope
if [ "$tsk_use_color" = true ]; then
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    NC='\033[0m' # No Color
    CR=$'\r'
else
    RED=''
    GREEN=''
    YELLOW=''
    NC=''
    CR=''
fi

# Benchmark timing variables
tsk_benchmark_enabled=false
tsk_benchmark_start_time=""
tsk_benchmark_total_time=0

# tests have their output redirected to files so we need to:
exec 3>&1 4>&2 # save stdout and stderr to FDs 3 and 4

. "$tsk_framework_dir/lib/test-api.sh"
. "$tsk_framework_dir/lib/run-testfiles-fork.sh"
. "$tsk_framework_dir/lib/arguments.sh"
. "$tsk_framework_dir/lib/formatters/dot.sh"
. "$tsk_framework_dir/lib/formatters/list.sh"
. "$tsk_framework_dir/lib/formatters/test.sh"


# Generates a list of test functions to run.
_find_tests() {
    # or: `declare -F | cut -d" " -f3 | grep '^test:'`
    compgen -A function test:
    compgen -A function skip:
    # TODO: should run the tests in the order they're declared
    # in the file, not in whatever alphabetical order compgen returns.
}


_print_test_error_messages() {
    if [ ${#tsk_test_error_messages[@]} -gt 0 ]; then
        for msg in "${tsk_test_error_messages[@]}"; do
            echo "  - $msg" >&3
        done
    fi
}


# runs a single test from a testfile
_run_test() {
    [ -n "$tsk_skip_test" ] && return
    exec 1>"$tsk_root_dir/test-stdout" 2>"$tsk_root_dir/test-stderr"

    if type before:each &>/dev/null; then
        before:each "$tsk_test_name"
    fi
    [ -n "$tsk_skip_test" ] && return   # skip message is already printed

    "$tsk_test_name" "$tsk_test_name"   # TODO: useful arguments?

    if type after:each &>/dev/null; then
        after:each "$tsk_test_name"
    fi
    _run_cleanup
    if ! is_empty "$tsk_root_dir/run"; then
        _add_error "test left files/dirs behind: $(ls -A "$tsk_root_dir/run")"
    fi

    exec 1>&3 2>&4 # restore asap so test framework errors aren't swallowed

    # Add this test's assertions to the total
    tsk_total_assertions=$((tsk_total_assertions + tsk_test_assertions))

    is_empty "$tsk_root_dir/test-mocks" || _cleanup_mocks

    if [ -s "$tsk_root_dir/test-stderr" ] && [ "$tsk_stderr_checked" = false ]; then
        _add_error_with_stderr "test produced stderr:"
    fi
}


_compute_test_result() {
    if [ $tsk_test_assertions -eq 0 ]; then
        # Test made no assertions so call it skipped
        tsk_skip_count=$((tsk_skip_count + 1))
        tsk_test_result="skip"
    elif [ ${#tsk_test_error_messages[@]} -eq 0 ]; then
        # Test passed (made assertions and no errors)
        tsk_pass_count=$((tsk_pass_count + 1))
        tsk_test_result="pass"
    else
        # test had one or more failing assertions
        tsk_fail_count=$((tsk_fail_count + 1))
        tsk_test_result="fail"
    fi
}


_run_testfile() {
    local tsk_testfile="$1"

    # in case the user named a ".skip" file on the command line
    tsk_skip_all_tests=""
    if [[ "$tsk_testfile" == *.skip ]]; then
        tsk_skip_all_tests=true
    fi

    # load the testfile if it's not already loaded
    if [ "$tsk_testfile" != "$tsk_launched_file" ]; then
        # remove the launched testfile's test functions
        for func in $(compgen -A function test:); do
            unset -f "$func"
        done
        # What about the before/after:all hooks?
        # Answer: right now they replace the launched testfile's functions.
        # But, if we convert them to hooks, we need to remove the launched testfile's hooks.

        # convert to aboslute path because we'll be running the test
        # in a different directory than the cwd
        tsk_testfile="$(readlink -f "$tsk_testfile")"
        source "$tsk_testfile"
    fi

    mkdir -p "$tsk_root_dir/run" 2>&4 || exit 1
    cd "$tsk_root_dir/run"

    # Run the before:all hook if defined
    if [ -z $tsk_skip_all_tests ] && type before:all &>/dev/null; then
        # TODO: where should stdout/stderr go for before/after:all?
        # TODO: what if before:all fails some assertions?
        before:all
        # If skip was called in before:all, skip all tests in this file
        if [ -n "$tsk_skip_test" ]; then
            tsk_skip_all_tests=true
        fi
    fi

    local tsk_test_name
    for tsk_test_name in $(_find_tests | $tsk_match_utility); do
        # Reset test-specific variables for each test
        tsk_test_assertions=0
        tsk_test_error_messages=()
        tsk_stderr_checked=false
        tsk_test_result=""
        tsk_skip_test="$tsk_skip_all_tests"
        tsk_cleanup_actions=()

        cd "$tsk_root_dir/run"
        tsk_total_count=$((tsk_total_count + 1))
        "_${tsk_test_formatter}_formatter_pre_test"
        if [[ "$tsk_test_name" == test:* ]]; then
            _run_test "$tsk_test_name"
        fi
        _compute_test_result
        "_${tsk_test_formatter}_formatter_post_test" "$tsk_test_result"
    done

    if [ -z $tsk_skip_all_tests ] && type after:all &>/dev/null; then
        # TODO: where should stdout/stderr go for before/after:all?
        # TODO: what if after:all fails some assertions?
        after:all
    fi
}


# TODO: test running tests in the testfile in parallel
# testing hierarchy:
# - tsk_root_dir         root for this particular testfile
#   - run                the current test's cwd, empty at start of test (TODO: parallel?)
#   - test-mocks         directory containing the current test's mocks
#   - test-stdout        file, the current test's stdout
#   - test-stderr        file, the current test's stderr

_prepare_test_environment() {
    tsk_total_count=0     # current number of tests run
    tsk_pass_count=0      # current number of tests passed
    tsk_fail_count=0      # current number of tests failed
    tsk_skip_count=0      # current number of tests skipped
    tsk_total_assertions=0 # total number of assertions made

    tsk_keep_artifacts=''
    tsk_use_diff=''
    tsk_match_utility='cat' # TODO: accepting code from user: make sure this isn't a vuln!

    # TODO: running in /tmp isn't good for tests that require giant files
    # TODO: option to create and mount a ramdisk to run tests

    # if we're a sub-test, the parent is telling us the directory to use
    if [ -n "$TSK_ROOT_DIR" ]; then
        tsk_root_dir="$TSK_ROOT_DIR"
        mkdir -p "$tsk_root_dir" || exit 1
    else
        tsk_root_dir="$(mktemp -d "/tmp/bashtest-XXXXXX")"
    fi

    local config_file="$(dirname "$tsk_framework_file")/test-config.sh"
    [ -f "$config_file" ] && source "$config_file"
}


_run_all_tests() {
    if [ "$tsk_benchmark_enabled" = true ]; then
        tsk_benchmark_start_time=$(date +%s%N)  # epoch seconds + nanoseconds
    fi

    # if the user supplies a single testfile, we show a more verbose output than if they're running multiple
    if [ ${#testfiles[@]} -eq 1 ]; then
        tsk_test_formatter="${tsk_test_formatter:-list}"
        _run_testfile "${testfiles[0]}"
    else
        # TODO: need to print failures and results when running multiple test files
        tsk_test_formatter="${tsk_test_formatter:-dot}"
        _run_testfiles
    fi

    _${tsk_test_formatter}_formatter_test_summary

    if [ "$tsk_benchmark_enabled" = true ] && [ -n "$tsk_benchmark_total_time" ]; then
        elapsed_ms=$(( ($(date +%s%N) - tsk_benchmark_start_time) / 1000000 ))
        tsk_benchmark_total_time=$(printf "%d.%03d" $((elapsed_ms / 1000)) $((elapsed_ms % 1000)))
        echo -e "${GREEN}Running${NC} ${#testfiles[@]} ${GREEN}$(pluralize ${#testfiles[@]} testfile) took${NC} ${tsk_benchmark_total_time} ${GREEN}seconds${NC}"
    fi
}


# TODO: if we're benchmarking, can we summarize the results?
_repeat_all_tests() {
    while [ "$tsk_repeat" -gt 0 ]; do
        ( _run_all_tests )
        local exit_code="$?"
        if [ "$exit_code" -ne 0 ]; then
            exit "$exit_code"
        fi

        # the fail-results and skip-results files get appended to
        # so they must be cleared before anything is repeated
        rm -f "$tsk_root_dir"/*/*-results

        tsk_repeat=$((tsk_repeat - 1))
    done
}

_prepare_test_environment
_process_arguments "$@"
_repeat_all_tests

# Clean up if not in watch mode and not keeping artifacts
if [ "$tsk_keep_artifacts" != true ]; then
    rm -rf "$tsk_root_dir"
else
    echo "test directory was $tsk_root_dir"
fi
