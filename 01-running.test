#!/bin/bash

# Tests what happens when running tests.
#   - test output
#   - ??

# allows for nicer syntax: now tests can "run_tests testfile"
run_tests() {
    export TSK_ROOT_DIR="$tsk_root_dir/run/bashtest"  # TODO: bad use of tsk_root_dir
    create_dir "$TSK_ROOT_DIR"    # directory is automatically cleaned up
    "$tsk_framework_file" "$@"
}

# a normal testfile used by many of these tests
create_run_test() {
    create_file run.test <<EOF
      : # this test is executed, not skipped
      : test:success() {
      :     is_eq 1 1
      : }
      :
      : test:fail() {
      :     is_eq 1 2
      : }
      :
      : skip:skipped() {
      :     is_eq 1 2
      : }
EOF
}

# Multithreaded tests can appear in any order
# So replaces, say, 'FF.s.s' with 'FFss..'
# Right now it's hard-coded to only support six tests.
fix_test_order() {
    sed 's/^[\.Fs]\{6\}$/FFss../'
}


# missing/invalid testfiles stop testing before any testfiles are loaded
test:missing_testfiles() {
    # because missing.test doesn't exist at all, if we attempt to even load
    # this testfile, that's an error.
    create_file existing.test <<EOL
        echo WRONG >&2
        exit 1'
EOL

    # list missing.test last so we're sure it's tested before anything gets loaded
    create_file out # TODO: turn this back into a pipeline.
    # right now it can't be a pipe because the first command is in a subshell, so
    # create_dir's cleanup task gets forgotten.
    run_tests existing.test missing.test > out
    cat out | sed 's/line [0-9][0-9]*/line XXX/g'
    # TODO: this error message needs to be improved. Including a line number makes no sense.
    stdout_is <<EOF
 00 ABORT  line XXX: Could not find missing.test
All testing aborted.
EOF
}

# ensure the dot formatter names the testfiles that had failed or skipped tests
test:dot_formatter_test_results() {
    create_run_test
    # run it twice so we get the multithreaded dot formatter
    run_tests run.test run.test | fix_test_order
    stdout_is <<EOF
FFss..
Failed tests:
  ✗ run.test: 1 test failed
  ✗ run.test: 1 test failed
Skipped tests:
  ○ run.test: 1 test skipped
  ○ run.test: 1 test skipped
EOF
}

# verify the benchmark feature exists
test:benchmark_works() {
    echo 'test:quick() { is_eq 1 1; }' > bench.test
    # Run with benchmark and verify output contains benchmark timing
    run_tests -b bench.test -f test | sed 's/[0-9][0-9]*\.[0-9][0-9]*/XX.XX/g'
    stdout_is <<EOL
pass test:quick
Running 1 testfile took XX.XX seconds
EOL

    rm bench.test
}

test:repeat_works() {
    echo 'test:quick() { is_eq 1 1; }' > repeat.test
    run_tests -r 3 repeat.test
    stdout_is <<EOL
 01 pass   test:quick
1 test: 1 passed, 0 failed.   1 assertion.
 01 pass   test:quick
1 test: 1 passed, 0 failed.   1 assertion.
 01 pass   test:quick
1 test: 1 passed, 0 failed.   1 assertion.
EOL

    rm repeat.test
}

test:repeating_a_failing_test_works() {
    echo 'test:quick() { is_eq 1 2; }' > repeat.test
    run_tests -r 3 -f test repeat.test
    stdout_is <<EOL
fail test:quick
  - line 1: is_eq expected '1', got '2'
fail test:quick
  - line 1: is_eq expected '1', got '2'
fail test:quick
  - line 1: is_eq expected '1', got '2'
EOL
    rm repeat.test
}

test:repeat_zero_times() {
    echo 'test:quick() { is_eq 1 1; }' > repeat.test
    run_tests -r 0 repeat.test
    stdout_is <<EOL
EOL
    rm repeat.test
}

test:repeat_negative_one_times() {
    echo 'test:quick() { is_eq 1 1; }' > repeat.test
    run_tests -r -1 repeat.test
    stderr_is <<EOL
invalid repeat argument: -1
EOL
    rm repeat.test
}

source "$(dirname "$BASH_SOURCE")/run-tests"
